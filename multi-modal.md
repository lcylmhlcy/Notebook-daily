# Top Conferences and Journals for Multi-modal
1. Vision[C]: CVPR, ICCV, ECCV
1. AI[C]: NIPS, AAAI, ICASSP, IJCAI
1. Multimedia[C]: ACMMM, ICME
1. Vision[J]: TPAMI, TIP, IJCV, PR
1. Multimedia[J]: TMM

---

# Survey

1. [Referring Expression Comprehension: A Survey of Methods and Datasets](https://arxiv.org/abs/2007.09554), Arxiv 2020.

1. [Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and Fusion](https://arxiv.org/abs/2006.08159), arxiv 2020.

1. [A survey on multi-modal social event detection](https://www.sciencedirect.com/science/article/pii/S0950705120301271), Knowledge-Based Systems 2020

1. [Visual question answering: A survey of methods and datasets](https://www.sciencedirect.com/science/article/pii/S1077314217300772), CVIU 2017.

---


# Grounding Relations / Referring Relations

1. [Referring relationships](https://arxiv.org/pdf/1803.10362.pdf), CVPR 2018. [[Code]](https://github.com/StanfordVL/ReferringRelationships) [[Website]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/index.html)

1. [Differentiable Scene Graphs](https://arxiv.org/pdf/1902.10200.pdf), arxiv 2019

1. [CPARR: Category-based Proposal Analysis for Referring Relationships](https://openaccess.thecvf.com/content_CVPRW_2020/html/w56/He_CPARR_Category-Based_Proposal_Analysis_for_Referring_Relationships_CVPRW_2020_paper.html), CVPR 2020 Workshop

1. [Learning latent scene-graph representations for referring relationships](https://www.researchgate.net/profile/Roei_Herzig/publication/331397250_Differentiable_Scene_Graphs/links/5c7d2e43458515831f826ad5/Differentiable-Scene-Graphs.pdf), Arxiv 2019

1. [Revisiting Visual Grounding](https://arxiv.org/pdf/1904.02225.pdf), Arvix 2019
	- Critique of Referring Relationship paper

1. [Visual Referring Expression Recognition: What Do Systems Actually Learn?](https://arxiv.org/abs/1805.11818), NAACL 2018 short.
	- Critique of Visual Grounding paper
---

# Multimodal Fusion

1. [Dynamic Fusion for Multimodal Data](https://arxiv.org/abs/1911.03821), arXiv 2019

1. [Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling](https://papers.nips.cc/paper/9381-deep-multimodal-multilinear-fusion-with-high-order-polynomial-pooling), NeurIPS 2019

1. [MFAS: Multimodal Fusion Architecture Search](https://arxiv.org/abs/1903.06496), CVPR 2019

1. [The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision](https://arxiv.org/abs/1904.12584), ICLR 2019 [[code]](http://nscl.csail.mit.edu/)

1. [Efficient Low-rank Multimodal Fusion with Modality-Specific Factors](https://arxiv.org/abs/1806.00064), ACL 2018 [[code]](https://github.com/Justin1904/Low-rank-Multimodal-Fusion)

1. [Memory Fusion Network for Multi-view Sequential Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17341/16122), AAAI 2018 [[code]](https://github.com/pliang279/MFN)

1. [Tensor Fusion Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/1707.07250), EMNLP 2017 [[code]](https://github.com/A2Zadeh/TensorFusionNetwork)

1. [Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework](http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf), AAAI 2015

---

# Visual Grounding / Referring Expression Comprehension (Images):

1. [Deep fragment embeddings for bidirectional image sentence mapping](http://papers.nips.cc/paper/5281-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping.pdf), CVPR 2016.

1. [Deep visual-semantic alignments for generating image descriptions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf), CVPR 2016. [[Code]](https://github.com/karpathy/neuraltalk) [[Torch Code]](https://github.com/karpathy/neuraltalk2) [[Website]](https://cs.stanford.edu/people/karpathy/deepimagesent/)

1. [Natural language object retrieval](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Hu_Natural_Language_Object_CVPR_2016_paper.pdf), CVPR 2016. [[Code]](https://github.com/ronghanghu/natural-language-object-retrieval) [[Website]](http://ronghanghu.com/text_obj_retrieval/)

1. [Generation and comprehension of unambiguous object descriptions](https://arxiv.org/pdf/1511.02283.pdf), CVPR 2016. [[Code]](https://github.com/mjhucla/Google_Refexp_toolbox)

1. [Learning deep structure-preserving image-text embeddings](http://slazebni.cs.illinois.edu/publications/cvpr16_structure.pdf), CVPR 2016. [[Code]](https://github.com/lwwang/Two_branch_network)

1. [Modeling context in referring expressions](https://arxiv.org/pdf/1608.00272.pdf), ECCV 2016. [[Code]](https://github.com/lichengunc/refer)

1. [Modeling context between objects for referring expression understanding](https://arxiv.org/pdf/1608.00525.pdf), ECCV 2016. [[Code]](https://github.com/varun-nagaraja/referring-expressions)

1. [Grounding of textual phrases in images by reconstruction](https://arxiv.org/pdf/1511.03745.pdf), ECCV 2016. [[Tensorflow Code]](https://github.com/kanchen-usc/GroundeR) [[Torch Code]](https://github.com/ruotianluo/refexp-comprehension)

1. [Structured matching for phrase localization](https://pdfs.semanticscholar.org/9216/2ec88ad974cc5082d9688c8bfee672ad59ad.pdf), ECCV 2016. [[Code]](https://github.com/princeton-vl/structured-matching)

1. [Segmentation from natural language expressions](https://arxiv.org/pdf/1603.06180.pdf), ECCV 2016. [[Code]](https://github.com/ronghanghu/text_objseg) [[Website]](http://ronghanghu.com/text_objseg/)

1. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](https://arxiv.org/pdf/1606.01847.pdf), EMNLP 2016. [[Code]](https://github.com/akirafukui/vqa-mcb)

1. [An attention-based regression model for grounding textual phrases in images](https://www.ijcai.org/proceedings/2017/0558.pdf), IJCAI 2017.

1. [An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning](https://arxiv.org/pdf/1703.07579.pdf), Arxiv 2017. [[Code]](https://github.com/jxwufan/NLOR_A3C)

1. [A joint speakerlistener-reinforcer model for referring expressions](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_A_Joint_Speaker-Listener-Reinforcer_CVPR_2017_paper.pdf), CVPR 2017. [[Code]](https://github.com/lichengunc/speaker_listener_reinforcer)[[Website]](https://vision.cs.unc.edu/refer/)

1. [Modeling relationships in referential expressions with compositional modular networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Modeling_Relationships_in_CVPR_2017_paper.pdf), CVPR 2017. [[Code]](https://github.com/ronghanghu/cmn)

1. [Comprehension-guided referring expressions](http://openaccess.thecvf.com/content_cvpr_2017/papers/Luo_Comprehension-Guided_Referring_Expressions_CVPR_2017_paper.pdf), CVPR 2017 [[Code]](https://github.com/ruotianluo/refexp-comprehension)

1. [Referring expression generation and comprehension via attributes](http://faculty.ucmerced.edu/mhyang/papers/iccv2017_referring_expression.pdf), CVPR 2017.

1. [Weakly-supervised visual grounding of phrases with linguistic structures](https://arxiv.org/pdf/1705.01371.pdf), Arxiv 2017.

1. [Phrase localization and visual relationship detection with comprehensive image-language cues](http://openaccess.thecvf.com/content_ICCV_2017/papers/Plummer_Phrase_Localization_and_ICCV_2017_paper.pdf), ICCV 2017. [[Code]](https://github.com/BryanPlummer/pl-clc)

1. [Query-guided regression network with context policy for phrase grounding](http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Query-Guided_Regression_Network_ICCV_2017_paper.pdf), ICCV 2017. [[Code]](https://github.com/kanchen-usc/QRC-Net)

1. [Recurrent Multimodal Interaction for Referring Image Segmentation](https://arxiv.org/pdf/1703.07939.pdf), ICCV 2017. [[Code]](https://github.com/chenxi116/TF-phrasecut-public)

1. [Deep attribute-preserving metric learning for natural language object retrieval](https://dl.acm.org/citation.cfm?id=3123439), ACMMM 2017.

1. [Bundled Object Context for Referring Expressions](https://ieeexplore.ieee.org/document/8307406), TMM 2018.

1. [Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding](https://www.ijcai.org/proceedings/2018/0155.pdf), Arxiv 2018. [[Code]](https://github.com/XiangChenchao/DDPN)

1. [Mattnet: Modular attention network for referring expression comprehension](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.pdf), CVPR 2018. [[Code]](https://github.com/lichengunc/MAttNet) [[Website]](http://vision2.cs.unc.edu/refer/comprehension)

1. [Visual Grounding via Accumulated Attention](http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf), CVPR 2018.

1. [Knowledge Aided Consistency for Weakly Supervised Phrase Grounding](https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Knowledge_Aided_Consistency_CVPR_2018_paper.html), CVPR 2018.

1. [Weakly Supervised Phrase Localization With Multi-Scale Anchored Transformer Network](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Weakly_Supervised_Phrase_CVPR_2018_paper.html), CVPR 2018.

1. [Referring image segmentation via recurrent refinement networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Referring_Image_Segmentation_CVPR_2018_paper.pdf), CVPR 2018. [[Code]](https://github.com/liruiyu/referseg_rrn)

1. [Grounding referring expressions in images by variational context](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Grounding_Referring_Expressions_CVPR_2018_paper.pdf), CVPR 2018. [[Code]](https://github.com/yuleiniu/vc/)

1. [Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining](https://arxiv.org/pdf/1808.00265.pdf), Arxiv 2018.

1. [Knowledge aided consistency for weakly supervised phrase grounding](https://arxiv.org/abs/1803.03879), Arxiv 2018. [[Code]](https://github.com/kanchen-usc/KAC-Net)

1. [Using syntax to ground referring expressions in natural images](https://arxiv.org/pdf/1805.10547.pdf), Arxiv 2018. [[Code]](https://github.com/volkancirik/groundnet)

1. [Dynamic multimodal instance segmentation guided by natural language queries](https://arxiv.org/pdf/1807.02257.pdf), ECCV 2018. [[Code]](https://github.com/BCV-Uniandes/DMS)

1. [Key-word-aware network for referring expression image segmentation](http://openaccess.thecvf.com/content_ECCV_2018/papers/Hengcan_Shi_Key-Word-Aware_Network_for_ECCV_2018_paper.pdf), ECCV 2018. [[Code]](https://github.com/shihengcan/key-word-aware-network-pycaffe)

1. [Conditional image-text embedding networks](https://arxiv.org/pdf/1711.08389.pdf), ECCV 2018. [[Code]](https://github.com/BryanPlummer/cite)

1. [Using Syntax to Ground Referring Expressions in Natural Images](https://arxiv.org/abs/1805.10547), AAAI 2018 [[code]](https://github.com/volkancirik/groundnet)

1. [Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding](https://arxiv.org/pdf/1811.11683v1.pdf), Arxiv 2018.

1. [PIRC Net: Using Proposal Indexing, Relationships and Context for Phrase Grounding](https://arxiv.org/pdf/1812.03213v1.pdf), Arxiv 2018.

1. [Real-Time Referring Expression Comprehension by Single-Stage Grounding Network](https://arxiv.org/pdf/1812.03426v1.pdf), Arxiv 2018.

1. [Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks](https://arxiv.org/pdf/1812.04794.pdf), Arxiv 2018.

1. [Visual Referring Expression Recognition: What Do Systems Actually Learn?](https://arxiv.org/abs/1805.11818), NAACL 2018 short.

1. [You Only Look & Listen Once: Towards Fast and Accurate Visual Grounding](https://arxiv.org/pdf/1902.04213.pdf), Arxiv 2019.

1. [Learning to Compose and Reason with Language Tree Structures for Visual Grounding](https://arxiv.org/pdf/1906.01784.pdf), TPAMI 2019.

1. [Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing](https://arxiv.org/pdf/1903.00839.pdf), CVPR 2019.

1. [Neural Sequential Phrase Grounding (SeqGROUND)](https://arxiv.org/pdf/1903.07669.pdf), CVPR 2019.

1. [Modularized textual grounding for counterfactual resilience](https://arxiv.org/pdf/1904.03589.pdf), CVPR 2019.

1. [Cross-Modal Self-Attention Network for Referring Image Segmentation](https://arxiv.org/pdf/1904.04745.pdf), CVPR 2019.

1. [Cross-Modal Relationship Inference for Grounding Referring Expressions](https://arxiv.org/pdf/1906.04464.pdf), CVPR 2019.

1. [Learning to Assemble Neural Module Tree Networks for Visual Grounding](http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper.pdf), ICCV 2019. [[Code]](https://github.com/daqingliu/NMTree)

1. [Align2ground: Weakly supervised phrase grounding guided by image-caption alignment](https://arxiv.org/pdf/1903.11649.pdf), ICCV 2019

1. [Dynamic Graph Attention for Referring Expression Comprehension](https://arxiv.org/pdf/1909.08164.pdf), ICCV 2019. [[Code]](https://github.com/sibeiyang/sgmn/tree/master/lib/dga_models)

1. [Phrase Localization Without Paired Training Examples](https://arxiv.org/abs/1908.07553), ICCV 2019. [[Code]](https://github.com/josiahwang/phraseloceval) 

1. [A Fast and Accurate One-Stage Approach to Visual Grounding](https://arxiv.org/pdf/1908.06354.pdf), ICCV 2019. [[Code]](https://github.com/zyang-ur/onestage_grounding)

1. [Zero-Shot Grounding of Objects from Natural Language Queries](https://arxiv.org/abs/1908.07129), ICCV 2019. [[Code]](https://github.com/TheShadow29/zsgnet-pytorch)

1. [Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding](https://arxiv.org/pdf/1908.10568.pdf), ICCV 2019. [[Code]](https://github.com/GingL/ARN)

1. [Referring Expression Object Segmentation with Caption-Aware Consistency](https://arxiv.org/abs/1910.04748), BMVC 2019. [[Code]](https://github.com/wenz116/lang2seg)

1. [Phrase Grounding by Soft-Label Chain Conditional Random Field](https://arxiv.org/pdf/1909.00301.pdf), EMNLP 2019. [[Code]](https://github.com/liujch1998/SoftLabelCCRF)

1. [Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding](https://arxiv.org/abs/1901.06595), arXiv 2019 [[code]](https://github.com/facebookresearch/binary-image-selection)

1. [Learning Cross-modal Context Graph for Visual Grounding](https://arxiv.org/pdf/1911.09042.pdf), AAAI 2020. [[Code]](https://github.com/youngfly11/LCMCG-PyTorch)

1. [Visual-Semantic Graph Matching for Visual Grounding](https://dl.acm.org/doi/abs/10.1145/3394171.3413902), ACMMM 2020

1. [Language-Aware Fine-Grained Object Representation for Referring Expression Comprehension](https://dl.acm.org/doi/abs/10.1145/3394171.3413850), ACMMM 2020

1. [Give Me Something to Eat: Referring Expression Comprehension with Commonsense Knowledge](https://arxiv.org/abs/2006.01629), ACMMM 2020

1. [Cascade Grouped Attention Network for Referring Expression Segmentation](https://dl.acm.org/doi/abs/10.1145/3394171.3414006), ACMMM 2020

1. [Transferrable Referring Expression Grounding with Concept Transfer and Context Inheritance](https://dl.acm.org/doi/abs/10.1145/3394171.3413677), ACMMM 2020

1. [Bi-Directional Relationship Inferring Network for Referring Image Segmentation](https://openaccess.thecvf.com/content_CVPR_2020/html/Hu_Bi-Directional_Relationship_Inferring_Network_for_Referring_Image_Segmentation_CVPR_2020_paper.html), CVPR 2020.

1. [A Real-Time Cross-Modality Correlation Filtering Method for Referring Expression Comprehension](https://openaccess.thecvf.com/content_CVPR_2020/html/Liao_A_Real-Time_Cross-Modality_Correlation_Filtering_Method_for_Referring_Expression_Comprehension_CVPR_2020_paper.html), CVPR 2020.

1. [What Does BERT with Vision Look At?](https://www.aclweb.org/anthology/2020.acl-main.469/), ACL 2020

---

# Related - VG (image)

1. [Chain of Reasoning for Visual Question Answering](https://proceedings.neurips.cc/paper/2018/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf), NPIS 2018.

1. [Grounded Language Learning Fast and Slow](https://arxiv.org/abs/2009.01719), arXiv 2020

1. [VIOLIN: A Large-Scale Dataset for Video-and-Language Inference](https://arxiv.org/abs/2003.11618), CVPR 2020 [[code]](https://github.com/jimmy646/violin)

1. [Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions](https://arxiv.org/abs/1811.10652), CVPR 2019

1. [Multilevel Language and Vision Integration for Text-to-Clip Retrieval](https://arxiv.org/abs/1804.05113), AAAI 2019 [[code]](https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval)

1. [Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos](http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Finding_It_Weakly-Supervised_CVPR_2018_paper.pdf), CVPR 2018

1. [Grounding Visual Explanations](https://link.springer.com/chapter/10.1007/978-3-030-01216-8_17), ECCV 2018.

1. [Gated-Attention Architectures for Task-Oriented Language Grounding](https://arxiv.org/abs/1706.07230), AAAI 2018 [[code]](https://github.com/devendrachaplot/DeepRL-Grounding)

1. [Grounding language acquisition by training semantic parsers using captioned videos](https://cbmm.mit.edu/sites/default/files/publications/Ross-et-al_ACL2018_Grounding%20language%20acquisition%20by%20training%20semantic%20parsing%20using%20caption%20videos.pdf), ACL 2018

1. [Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts](https://arxiv.org/abs/1803.11209), NeurIPS 2017

---

# References
1. https://github.com/pliang279/awesome-multimodal-ml#multimodal-fusion
1. https://github.com/Eurus-Holmes/Awesome-Multimodal-Research
1. https://github.com/TheShadow29/awesome-grounding
1. https://github.com/iworldtong/Awesome-Temporal-Sentence-Grounding-in-Videos
1. https://github.com/WuJie1010/Temporally-language-grounding
